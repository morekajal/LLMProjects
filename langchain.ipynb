{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPEN_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Your LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = OpenAI(temperature=0.6)\n",
    "# This throws error for key not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=os.environ[\"OPEN_API_KEY\"], temperature=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temperature => Decides creativity of our model. 0 (low risk) to close to 1 (high risk) of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the Capital of India\"\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How to deal with Hugging face open source models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain is just used as a wrapper, it can call OpenAI, LLM models, Hugging face models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MyStudy\\llm\\llm_langchain_openai_tutorials\\krish_langchain_series\\Langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\MyStudy\\llm\\llm_langchain_openai_tutorials\\krish_langchain_series\\Langchain\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "llm_huggingface = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calcutta\n"
     ]
    }
   ],
   "source": [
    "output = llm_huggingface.predict(\"Can you tell me the capital of India?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see, Hugging face is giving a short answer and the answer may not be correct always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rain is a beautiful thing\n"
     ]
    }
   ],
   "source": [
    "output = llm_huggingface.predict(\"Can you write poem about rain?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nRain falls from the sky\\nIt fills the air with its sighs\\nThe water droplets make their way\\nDown to the earth, and here they stay\\n\\nThe sound of its patter\\nEchoes throughout the land\\nBringing life to the soil\\nAnd nourishment to the sand\\n\\nThe grey clouds that bring it\\nAre a reminder of its might\\nThat it can come down hard\\nOr be a gentle light\\n\\nRain is a blessing\\nIt's beauty, we can't ignore\\nIt's power, we can't deny\\nAs it washes us to the core\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"Can you write poem about rain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the llm model gives better response than hugging face models. (The paid hugging face models can also give better results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates and LLM Chain:\n",
    "- To define how our input and output should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me the capital of the country India'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate (input_variables=['country'],\n",
    "                                  template = \"Tell me the capital of the country {country}\")\n",
    "\n",
    "prompt_template.format(country = \"India\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following is not the correct way to give input in a template, it throws error\n",
    "# llm.predict(prompt_template=prompt_template, text=[\"India \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "print(chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Multiple Chains using Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_prompt = PromptTemplate(input_variables=['country'],\n",
    "                                template=\"Please tell me the capital of {country}\")\n",
    "\n",
    "capital_chain = LLMChain(llm=llm, prompt = capital_prompt)\n",
    "\n",
    "# creating one more template with same input\n",
    "famous_template = PromptTemplate(input_variables=['capital'],\n",
    "                                 template = \"Suggest me some amazing places to visit in {capital}\")\n",
    "\n",
    "famous_chain = LLMChain(llm=llm, prompt=famous_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To give the template one after another to next prompt, will use Sequential chain\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-P3zz0utKN7KVRgIRbLktoL1Z on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-P3zz0utKN7KVRgIRbLktoL1Z on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-P3zz0utKN7KVRgIRbLktoL1Z on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-P3zz0utKN7KVRgIRbLktoL1Z on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some amazing places to visit in New Delhi: \n",
      "\n",
      "1. Red Fort: The majestic Red Fort is a UNESCO World Heritage Site and one of the iconic landmarks of India. It is a symbol of Mughal grandeur and a reminder of India’s colonial past. \n",
      "\n",
      "2. Qutub Minar: The world-famous Qutub Minar is a 73-meter tall tower constructed by Qutub-ud-din Aibak in 1193. This is a UNESCO World Heritage Site and one of the most visited tourist sites in India. \n",
      "\n",
      "3. Humayun's Tomb: This is a beautiful mausoleum built by Humayun's wife in 1570. It is a UNESCO World Heritage Site and an important landmark in Indian history. \n",
      "\n",
      "4. Jama Masjid: This is one of the largest mosques in India and is a popular tourist site. It was built by Mughal Emperor Shah Jahan in 1656. \n",
      "\n",
      "5. India Gate: India Gate is a war memorial located in the heart of New Delhi. It is a popular tourist spot and a symbol of India's freedom struggle. \n",
      "\n",
      "6. Akshardham Temple: This is\n"
     ]
    }
   ],
   "source": [
    "chain = SimpleSequentialChain(chains= [capital_chain, famous_chain ])\n",
    "print(chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output didnt tell us the specific sentence answer for our first template. For that we have to use buffer memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_prompt = PromptTemplate(input_variables=['country'],\n",
    "                                template=\"Please tell me the capital of {country}\")\n",
    "\n",
    "capital_chain = LLMChain(llm=llm, prompt = capital_prompt, output_key=\"capital\")\n",
    "\n",
    "famous_template = PromptTemplate(input_variables=['capital'],\n",
    "                                 template = \"Suggest me some amazing places to visit in {capital}\")\n",
    "\n",
    "famous_chain = LLMChain(llm=llm, prompt=famous_template, output_key=\"places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-P3zz0utKN7KVRgIRbLktoL1Z on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-P3zz0utKN7KVRgIRbLktoL1Z on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-P3zz0utKN7KVRgIRbLktoL1Z on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-P3zz0utKN7KVRgIRbLktoL1Z on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-P3zz0utKN7KVRgIRbLktoL1Z on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-P3zz0utKN7KVRgIRbLktoL1Z on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': 'India', 'capital': '\\n\\nThe capital of India is New Delhi.', 'places': \" Here are some amazing places to visit in New Delhi:\\n\\n1. Red Fort: This majestic fort stands as a symbol of India's rich cultural and architectural heritage.\\n\\n2. India Gate: This iconic monument is a memorial to the Indian soldiers who lost their lives in World War I.\\n\\n3. Jama Masjid: This is one of the largest mosques in India and is a must-visit for its intricate architecture and vibrant atmosphere.\\n\\n4. Humayun's Tomb: This magnificent Mughal era structure is a World Heritage Site and a must-visit to appreciate its beauty.\\n\\n5. Lodhi Gardens: This sprawling garden is a great place to relax and enjoy nature.\\n\\n6. Qutub Minar: This is the tallest brick minaret in the world and is a great place to explore and admire its architectural beauty.\\n\\n7. Akshardham Temple: This is a modern Hindu temple complex and is a must-visit for its beautiful architecture and spiritual atmosphere.\\n\\n8. Chandni Chowk: This bustling market is a great place to explore Delhi's culture and shop for souvenirs.\\n\\n9. Raj Ghat: This is the memorial of Mahatma Gandhi\"}\n"
     ]
    }
   ],
   "source": [
    "chain = SequentialChain(chains= [capital_chain, famous_chain ],\n",
    "                        input_variables=['country'],\n",
    "                        output_variables=['capital', 'places'])\n",
    "# print(chain.run(\"India\"))   #for sequential simple chain.run is not used\n",
    "print(chain({'country':\"India\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatmodels with ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm = ChatOpenAI(openai_api_key=os.environ[\"OPEN_API_KEY\"], temperature=0.6, model = 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. What is AIML and what does it stand for?\\n2. How does AIML differ from other programming languages?\\n3. What are the main components of an AIML file?\\n4. How does AIML utilize pattern matching and template responses?\\n5. Can you explain the concept of categories in AIML?\\n6. How does AIML handle user inputs and generate appropriate responses?\\n7. Can you describe the AIML pattern matching algorithm?\\n8. What are some advantages of using AIML for creating chatbots or virtual assistants?\\n9. Are there any limitations or challenges associated with using AIML?\\n10. How can AIML be extended or customized to improve its functionality?\\n11. Can you provide examples of AIML code snippets and explain their purpose?\\n12. How does AIML handle context and maintain conversation flow?\\n13. What are some popular AIML platforms or frameworks available in the market?\\n14. Can AIML be used for natural language processing and understanding?\\n15. How does AIML handle ambiguous or open-ended user queries?\\n16. What are some techniques for training an AIML chatbot?\\n17. Can AIML be integrated with other technologies or APIs?\\n18. How does AIML handle multi-language support?\\n19. What are some best practices for designing AIML-based chatbots?\\n20. Can you explain the concept of AIML sets and maps and their usage in chatbot development?')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create schema in a list\n",
    "\n",
    "chatllm([\n",
    "SystemMessage(content= \"You are an AIML Interviewer\"),\n",
    "HumanMessage(content= \"Please provide some questions about AIML\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Template + LLM + Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commaseparatedoutput(BaseOutputParser):\n",
    "    def parse(self, text:str):\n",
    "        return text.strip().split(\" , \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define template to make comma separated string\n",
    "template = \" Your are a helpful assisatent. When the user gives any input, you should generate 5 relatable words which should ne comma separated.\"\n",
    "human_template = \"{text}\"\n",
    "chatpromt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chatpromt|chatllm|Commaseparatedoutput()    #chain up operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Smart, Clever, Brilliant, Genius, Wise']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":\"Intelligent\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
